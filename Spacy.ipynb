{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPAShU+eP6n3YMYbXWUbP4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/Spacy/blob/main/Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's $\\color{red}{\\text{spaCy}}$?\n",
        "\n",
        "SpaCy is **free**, **open-source library** for advanced **Natural language processing**(NLP) in Python.\n",
        "\n",
        "Suppose you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What does the words mean in the context? Who is doing what to whom? What products and compnaies are mentioned in the text? Which texts are simmilar to each other.\n",
        "\n",
        "spaCy is designed specifically for **production use** and helps you build applications that process and \"understand\" large volume of text. It can be used to build **information extraction** or **natural language processing** systems, or to pre-process text for **deep learning**.\n",
        "\n"
      ],
      "metadata": {
        "id": "S0jw613b9IUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhRHxk1M7aGe"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "# Text: The original word text.\n",
        "# Lemma: The base form of the word.\n",
        "# POS: The simple part-of-speech tag.\n",
        "# Tag: The detailed part-of-speech tag.\n",
        "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "# Shape: The word shape – capitalization, punctuation, digits.\n",
        "# is alpha: Is the token an alpha character?\n",
        "# is stop: Is the token part of a stop list, i.e. the most common words of the language?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\color{red}{\\text{Tokenization}}$\n",
        "\n",
        "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token.  Each *Doc* consists of individual tokens, and we can iterate over them:"
      ],
      "metadata": {
        "id": "tMvU1CvO-fZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "3hY8lPpP7hei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "5K2LmqPg7hhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using spaCy’s built-in **displaCy** visualizer, here’s what our example sentence and its dependencies look like:\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Google, Apple crack down on fake coronavirus apps\")\n",
        "#displacy.serve(doc, style=\"dep\")\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 150})"
      ],
      "metadata": {
        "id": "FFS360ui7hkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{red}{\\text{Named Entities }}$ \n",
        "\n",
        "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
        "\n",
        "Named entities are available as the ents property of a Doc:"
      ],
      "metadata": {
        "id": "kYIk4BrXBiJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "4LASA6uI7hnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{en_core_web_sm}}$\n",
        "\n",
        "\n",
        "LANGUAGE ------> **en** English\n",
        "\n",
        "TYPE   ------> **core**  Vocabulary, syntax, entities\n",
        "\n",
        "GENRE ------> **web**  written text (blogs, news, comments)\n",
        "\n",
        "SIZE------> **sm** 12 MB"
      ],
      "metadata": {
        "id": "5fFayOl_CAhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the $\\color{red}{\\text{Named Entity recognizer}}$\n",
        "\n",
        "The entity visualizer, *ent* , highlight named entities and their label in the text."
      ],
      "metadata": {
        "id": "4to-lwczDfEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from spacy import displacy\n",
        "\n",
        "text = \"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style='ent', jupyter=True)\n",
        "#displacy.serve(doc, style=\"ent\")\n",
        "# https://spacy.io/api/annotation#named-entities"
      ],
      "metadata": {
        "id": "MLctw_167hr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Words vector and similarity\n",
        "\n",
        "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word.Word vectors can be generated using an algorithm like word2vec and usually look like this:\n",
        "\n",
        "__Important_note:__ To make them compact and fast, spaCy's small models(all the pacakages end with sm) **don't ship with the word vectors**, and only include context-sensitive tensors. This means you can still use the similarity() to compare documents, tokens and spans - but result won't be as good, and individual tokens won't have any vectors is assigned. So, in orders to use *real* word vectors, you need to download a larger model:\n"
      ],
      "metadata": {
        "id": "A2GtYgB6EgPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "metadata": {
        "id": "Q726z8H77hvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = nlp(\"lion bear apple banana fadsfdshds\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "# Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared)\n",
        "# has vector: Does the token have a vector representation?\n",
        "# OOV: Out-of-vocabulary"
      ],
      "metadata": {
        "id": "iCIIciJI7hyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
        "tokens = nlp(\"lion bear cow apple mango spinach\")\n",
        "\n",
        "for token11 in tokens:\n",
        "    for token13 in tokens:\n",
        "        print(token11.text, token13.text, token11.similarity(token13))"
      ],
      "metadata": {
        "id": "2u9802sA7h1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "\n",
        "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
        "<img src='/content/18.png'>\n",
        "<img src=\"19.png\">\n",
        "\n"
      ],
      "metadata": {
        "id": "uaxHSGnOFagC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab, hashes and lexemes\n",
        "\n",
        "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab, that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values – in this case for example, “coffee” has the hash 3197928453018144401. Entity labels like “ORG” and part-of-speech tags like “VERB” are also encoded. Internally, spaCy only “speaks” in hash values.\n",
        "\n",
        "<img src=\"/content/20.png\">"
      ],
      "metadata": {
        "id": "iOQEJfbjI3nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
      ],
      "metadata": {
        "id": "7aszf7cR7h4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I love tea, over coffee\")\n",
        "print(doc.vocab.strings[\"tea\"]) # 6041671307218480733\n",
        "\n",
        "\n",
        "print(doc.vocab.strings[6041671307218480733])"
      ],
      "metadata": {
        "id": "jwlxOMST7h80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee!\")\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    # print(lexeme)\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "metadata": {
        "id": "pLWmeF_O7h_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee\")  # Original Doc\n",
        "print(doc.vocab.strings[\"tea\"])  # 6041671307218480733\n",
        "print(doc.vocab.strings[6041671307218480733])  # 'tea' \n",
        "\n",
        "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
        "# empty_doc.vocab.strings[6041671307218480733] will raise an error :(\n",
        "\n",
        "empty_doc.vocab.strings.add(\"tea\")  # Add \"tea\" and generate hash\n",
        "print(empty_doc.vocab.strings[6041671307218480733])  # 'tea' \n",
        "\n",
        "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
        "print(new_doc.vocab.strings[6041671307218480733])  # 'tea' 👍"
      ],
      "metadata": {
        "id": "eDgQbQHI7iCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{red}{\\text{Knowledge base}}$ \n",
        "\n",
        "To support the entity linking task, spaCy stores external knowledge in a KnowledgeBase. The knowledge base (KB) uses the Vocab to store its data efficiently.\n",
        "\n",
        "A knowledge base is created by first adding all entities to it. Next, for each potential mention or alias, a list of relevant KB IDs and their prior probabilities is added. The sum of these prior probabilities should never exceed 1 for any given alias."
      ],
      "metadata": {
        "id": "OjTPlHZsKZq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "# load the model and create an empty KB\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
        "\n",
        "print()\n",
        "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
        "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2"
      ],
      "metadata": {
        "id": "JkFCXeu77iFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "\n",
        "candidates = kb.get_alias_candidates(\"Douglas\")#get_alias_candidates\n",
        "for c in candidates:\n",
        "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
      ],
      "metadata": {
        "id": "0fA1upMz7iIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCp0hiwp7iKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COShyAMa7iNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}